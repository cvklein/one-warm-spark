---
title: "Explaining the joke"
date: 2025-11-17
---
[Yesterday's post](https://cvklein.github.io/2025/11/16/AI-Assessment/) about AI and assessment was prompted by being sent, for the hundredth time, something that said "Well, AI is here to stay, so we have to teach students how to use it."

I think this is naïve, in two respects.

First, generative AI is here to stay. But that's not the same as saying that cheap, good, easily accessible AI is here to stay.

If you're imagining a future where your students get cheap, good, easily accessible AI for free, ask yourself: who's making a profit in that scenario? How do the people who sunk an enormous amount of venture capital on AI making their money back.

The truth is that generative AI is *staggeringly* expensive to run. My story was partly inspired by [recent](https://pluralistic.net/2025/09/27/econopocalypse/) [reports](https://www.wheresyoured.at/make-fun-of-them/) that OpenAI is losing billions of dollars a year on inference. If you're not familiar with the jargon, inference is actually running the AI model to get the answer. So there are staggering losses involved in just delivering the core product -- not training new models, not salaries, not infrastructure.  *Just doing the thing* is an insatiable money pit.

But technology will save us, Colin! Costs of inference will go down, new chips, blah blah.  I mean, sure. Maybe electricity will be too cheap to meter too. The problem is that there are hard scaling laws at work here. Models get better by making the context windows larger.  Making context windows larger scales badly. That's a fact about how the transformer architecture works.  That's why current models chew through electricity and GPUs like Cookie Monster.

But ok, put that aside. Pretend tech is magic.  No matter how cheap things get, someone's gotta get paid. Right now we're in the stage where venture capital is shovelling money as fast as it can to effectively subsidise the staggering losses of AI. They're not doing that because they're nice, or because they like your students. They're doing it in the hopes of getting even more money back. Many of them will be disappointed. But eventually, someone's gotta get paid.

Now, that could end up looking a lot of different ways. The story assumes that OpenAI ends up like Adobe or Microsoft. Your University is on the hook for a site license, or you're an independent professional and get screwed by weird subscription fees.  But unlike (say) Microsoft Word, there won't be easy cheap alternatives: remember, part of the problem is that *just doing the thing* is expensive. Free models will go out of date quickly, or will only run with short context windows, or generally suck. Otherwise, you get what the modern internet excels at: ad-supported bullshit.

The world we're preparing students for is not the one we have now. It's one where people charge money for AI. As much money as possible. Remember, there are lots of awesome things that could be done, easily, with current technology that we don't do. Why? Because capital needs return on investment.  When I am in grumbly old man mood, I tell my students about the halcyon days of 2006, when you could get basically any movie ever made from Netflix. Gosh, that was great. We could do that now! The reason why we don't is not  technology,  but the iron demands of capitalism.  And if anyone tells you different: *cui bono*?

Second point of naïveté, following off from the first. There's this idea that we somehow need to teach students how to use the technology. Now, there are a few ways to take this. I think sometimes people think something like  "wow, a new technology, the kids don't know how to use it! Maybe we need to teach them, like, the ins and outs of prompt engineering and such." "

Pshaw. The kids know how to do this. They use it more than you do. Way more. Also, chat-based LLMs are made to be deceptively easy to use: the whole problem with hallucinations is that it's very easy to get LLMs to confidently spit out bullshit at the drop of a hat.

It's not that new technology doesn't require big instructional changes sometimes. My father was a landscape architect.  He was trained in hand drafting. (He had a beautiful drafting table, full of all kinds of neat little bits and bobs for measuring perimeters and those triangular scale rulers. His handwriting was great.) Mid-career, CAD got too big to ignore. So he retrained. Later, when he became a professor and started teaching,  he waxed nostalgic about the value of starting with hand drafting before you moved to CAD. You didn't get the same feel for drawing that he had learned, he said. But he didn't push it.  Kids started with CAD from year one, day one; that was the modern world, and they needed to start early to master the tech.

LLMs are not like that. LLMs are like iPads. You pick one up, you use it, you fiddle with it, it's easy. We do not spend any time telling students how to use iPads.  A lot of smart people were paid a lot of money to make it thus. Maybe in one of the possible future cyberpunk dystopias where your salary is docked for every token over your allocation?  Sure, then let's teach them to keep it tight. But for now, there's no tech problem, and if there is, you should ask your students how to solve it.

The smarter thing to mean by this something more like "Hey, students, use this the way *I* use it. Don't believe all the bullshit it spits out. Check your sources! etc!" People who say that we should teach the kids how to use ChatGPT forget that they already have these skills, and they learned them in a different context.

 The technological analogue is a bit like calculators[^1]: you need to learn how to do the thing so you know when to do the thing and what failure to do the thing looks like, and and once you do that you get an extra-fast tool to help you out.

But of course, the analogy is there to make the problem obvious: *you shouldn't teach students arithmetic by using calculators*. You teach arithmetic first. Then you add calculators for the stuff they've mastered to make it faster and more reliable. If they start with calculators, they're not going to understand what they're doing. That's why we still teach arithmetic: not just in spite of calculators, but precisely so people know how to do the right thing when they have one.

Here's another way to put the same point. Suppose you had a perfect oracle, who would answer all the questions you put to it at no cost to you. Sometimes a bit touchy about phrasing, true, but phrase things correctly and you get back whatever you want to know. Does anyone need to teach you how to use the oracle? No, *because you can just ask it how it should be used*. It's going to be its own best instructor. If ChatGPT ends up like that, we're out of a job. We don't need to teach students how to do anything.

But if ChatGPT is not a perfect oracle---and it very much isn't---then the best thing we can do, by a long shot, is teach the skills that will, incidentally, let you realise when ChatGPT is making shit up, or has written facile and content-less prose, or whatever.

I'm not saying that we need to teach things in the same way, or using the same assessment structures, or whatever. What we had before often didn't work terribly well. But the idea that we need to suddenly drop everything and reconfigure University education is not a *fait accompli*. It's a sales pitch. And it's a sales pitch made by people who are desperate to sell you access, at extortionate prices, to the very thing they tell you is the future.




[^1] I stole the analogy from my wife. Thanks, dear!

Permalink: [{{ site.url }}{{ page.url }}]({{ site.url }}{{ page.url }})
